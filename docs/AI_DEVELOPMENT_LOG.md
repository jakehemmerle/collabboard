# AI Development Log — CollabBoard

## Tools & Workflow

### AI Coding Tools Used

| Tool | Role | How It Was Integrated |
|------|------|----------------------|
| Claude Code | Primary development agent | Used for architecture planning, implementation across ~20 epics, test writing, debugging, and deployment. Drove ~90% of code generation. |
| Codex CLI | Architecture & research | Used for high-level design decisions, evaluating tech stack options (Vercel AI SDK vs LangChain, Firebase vs custom WebSocket), and drafting the implementation plan. |
| Cursor | IDE-integrated AI assistant | Used for quick text refactors, and targeted edits during development. |

### Workflow

1. **Planning phase**: Used Claude chat to evaluate architecture options, resulting in `docs/PLAN_FINAL_OPTION_1.md` — a 20-story plan across 6 epics with dependency graph.
2. **Implementation phase**: Fed each story's spec to Claude Code, which generated code following the established patterns. This happened for each Epic.
3. **Testing phase**: Claude Code wrote unit tests alongside each major feature. Roughly after each Epic, an E2E test was performed using Claude in the browser.
4. **Debugging phase**: Used Claude in the browser to narrow down where the errors were and fix them in the codebase.
5. **Deployment phase**: Claude Code configured Firebase Hosting, Cloud Functions, and Firestore rules. (see skills)

---

## MCP Usage

| MCP | What It Enabled |
|-----|----------------|
| Claude-in-Chrome | E2E browser testing — controlled Chrome for collaboration testing with developer (cursor sync verification, real-time object sync) |

---

## Effective Prompts

### Prompt 1: E2E Testing Prompt Reference

The initial prompt referenced the `docs/e2e-prompts` folder containing the set of prompts used to drive end-to-end (E2E) testing using Claude Code in Chrome. These prompts coordinated browser sessions with the user, simulating real-time cursor synchronization, and verifying state consistency between users.

**Why it worked**: This allowed me to be more hands off during the development process and NOT overspecify tests upfront.

### Prompt 2: Local Dev Environment Spin-Up (dev Skill)

The next most effective prompting pattern used the `dev` skill to spin up the full local development environment. This skill handled starting all Firebase local emulators (Auth, Firestore, RTDB), the Vite dev server, and performed pre-flight health checks.

```
/dev
```

**Why it worked**: Sometimes the agent tried to spin things up twice or didn't spin up the emulators. This prompt automated the bootstrapping of the whole project stack, providing a realistic testbed for browser and agent workflows. Ensured integration testing could happen against a local, safe environment mirroring production.

### Prompt 3: Deploy and Smoke Test (ship Skill)

Deployment and verification were driven by the `ship` skill, which orchestrated typechecking, linting, full build, tests, and finally deployed the latest main branch to production.

**Prompt Example** (ship skill):

```
/ship
```

**Why it worked**: The ship skill wrapped all core workflow commands so it wouldn't get hung up in CI.

---

## Code Analysis

| Category | AI-Generated | Hand-Written | Notes |
|----------|-------------|--------------|-------|
| Domain logic (intent handlers, geometry, routing) | 100% | 0% | All domain code was generated by the AI, following the implementation plans created collaboratively via interactive sessions and research. |
| UI components (*Shape.tsx, panels) | 100% | 0% | UI components were fully AI-generated following agreed architectural and design patterns. |
| Infrastructure (Firestore sync, presence, auth) | 100% | 0% | The AI implemented all infrastructure and integration code based on the session-guided plans. |
| Cloud Functions (AI chat, tools) | 100% | 0% | All related serverless and AI integration code was AI-generated from the collaboratively designed plans. |
| Tests | 100% | 0% | All tests were generated by the AI from specifications. |
| Config (Firebase, Vite, CI) | 100% | 0% | All configuration, including deployment and CI/CD, was AI-generated, refined interactively. |
| **Overall** | **100%** | **0%** | The entire codebase was generated by the AI from high-level plans, architecture, and research developed with the user. |

---

## Strengths & Limitations

### Where AI Excelled

- Planning
- Implementation
- **Scaffolding & boilerplate**: Generated entire module structures (contracts, handlers, components, tests) in the established patterns consistently.
- **Test generation**: Produced comprehensive test suites covering edge cases (13 viewport culling tests, connector routing with various configurations, clipboard with multi-object paste).
- **Integration code**: Wired up Vercel AI SDK, Firebase Cloud Functions, and LangSmith observability with correct configuration on first pass.
- **Planning**: Produced a well-structured 20-story plan with realistic dependency ordering.

### Where AI Struggled

- Codex under-specifies plans, Claude is a lot more verbose.
- Introducing bugs it was confident about or didn't test properly.
- Starts working on work it wasn't told to work on.
- **Firebase Auth in E2E testing**: Could not solve the two-user-single-browser problem without significant manual investigation (IndexedDB session sharing, inMemoryPersistence workaround).
- **Configuration debugging**: Firebase deploy errors (cleanup policy, function region, CORS) required iterative manual fixes.

---

## Key Learnings

1. **Pattern-first architecture pays off**: Establishing clear patterns early (discriminated unions, intent handlers, module structure) allowed the AI to replicate them accurately across all 20 stories. The more predictable the codebase, the more effective AI code generation becomes.

2. **AI works best with bounded, well-specified tasks**: Feeding one story at a time with explicit file lists and acceptance criteria produced better results than broad "implement the whole feature" prompts, UNLESS the story gets overspecified or has errors. **This is a balance that you can only learn by working with the agents.**

3. **AI can't debug what it can't see**: Real-time sync issues, streaming failures, and deploy errors all required browser DevTools, Firebase console, and manual network inspection. AI tools are blind to runtime behavior.

4. **Test-driven AI development works**: Having the AI write tests UPFRONT catches bugs early and provided regression safety. The 10 test files became a contract that subsequent changes had to respect.

5. **Know when to stop prompting and start debugging**: When the AI gives the same wrong answer twice, it's time to investigate manually. This was especially true for the Firebase Auth E2E testing problem and the streaming architecture issue.

6. **It doesn't work until its deployed and working in prod.**
